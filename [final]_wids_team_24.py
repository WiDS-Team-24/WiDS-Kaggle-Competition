# -*- coding: utf-8 -*-
"""{An] WiDS Team 24.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VC5IiR-iy8P9xO5yubYnesnICQym7pFd

##**1. Access Datasets from Google Colab**
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install optuna

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import xgboost as xgb
from sklearn.impute import KNNImputer
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import f1_score
from sklearn.preprocessing import StandardScaler, MinMaxScaler
import lightgbm as lgb
from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV
from sklearn.decomposition import PCA
import optuna
from sklearn.feature_selection import VarianceThreshold
from sklearn.model_selection import train_test_split, StratifiedKFold
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier, StackingClassifier

# load TRAIN datasets
train_connectome = pd.read_csv('/content/drive/MyDrive/Spring AI Studio Team 24 WiDS/TRAIN/TRAIN_FUNCTIONAL_CONNECTOME_MATRICES.csv')
train_categorical = pd.read_excel('/content/drive/MyDrive/Spring AI Studio Team 24 WiDS/TRAIN/TRAIN_CATEGORICAL_METADATA.xlsx')
train_solutions = pd.read_excel('/content/drive/MyDrive/Spring AI Studio Team 24 WiDS/TRAIN/TRAINING_SOLUTIONS.xlsx')
train_quantitative = pd.read_excel('/content/drive/MyDrive/Spring AI Studio Team 24 WiDS/TRAIN/TRAIN_QUANTITATIVE_METADATA.xlsx')

# load TEST datasets
test_quantitative = pd.read_excel('/content/drive/MyDrive/Spring AI Studio Team 24 WiDS/TEST/TEST_QUANTITATIVE_METADATA.xlsx')
test_connectome = pd.read_csv('/content/drive/MyDrive/Spring AI Studio Team 24 WiDS/TEST/TEST_FUNCTIONAL_CONNECTOME_MATRICES.csv')
test_categorical = pd.read_excel('/content/drive/MyDrive/Spring AI Studio Team 24 WiDS/TEST/TEST_CATEGORICAL.xlsx')


### Explore the data
# display a preview of train datasets
print("Display first 5 rows of TRAIN datasets")
train_quantitative.head()
train_connectome.head()
train_categorical.head()
train_solutions.head()


# info of datasets
print("Exploring info of TRAIN datasets")
train_quantitative.info()
train_connectome.info()
train_categorical.info()
train_solutions.info()

# describe the TRAIN datasets
print("Describe TRAIN datasets")
train_quantitative.describe()
train_connectome.describe()
train_categorical.describe()
train_solutions.describe()


# display a preview of test datasets
test_quantitative.head()
test_connectome.head()
test_categorical.head()


# describe the TRAIN datasets
print("Describe TEST datasets")
test_quantitative.describe()
test_connectome.describe()
test_categorical.describe()

"""##**2. Data cleaning**

### 1. train_quantitative and test_quantitative dataset

Check if any values are missing in the dataset
"""

train_quantitative_missing_val = train_quantitative.isna().sum()
train_quantitative_missing_percentage = train_quantitative_missing_val / len(train_quantitative) * 100
print({"Missing values for train quantitative is \n": train_quantitative_missing_percentage})

"""Check the distribution of values before processing (for MRI_Track_Age_at_Scan)



"""

sns.histplot(train_quantitative['MRI_Track_Age_at_Scan'], kde=True)
plt.title("Distribution of MRI_Track_Age_at_Scan (Before)")
plt.xlabel("Age at Scan")
plt.show()

"""Using KNN to impute the data for this particular column, since using average will create a spike, which will impact the score of the model.

"""

# Select only numerical features for KNN imputation, exclude Participant_id
numeric_features = train_quantitative.select_dtypes(include=['float64', 'int64'])

# apply standardScaler to ensure all features contribute equally
scaler = StandardScaler()  # change this to MinMaxScaler() if needed
scaled_features = scaler.fit_transform(numeric_features)
# Initialize and apply KNN imputer
imputer = KNNImputer(n_neighbors=11)
imputed_data = imputer.fit_transform(numeric_features)

# Convert the imputed data back to a DataFrame
train_quantitative_imputed = pd.DataFrame(imputed_data, columns=numeric_features.columns)

# Update the original DataFrame with the imputed values
train_quantitative['MRI_Track_Age_at_Scan'] = train_quantitative_imputed['MRI_Track_Age_at_Scan']


# Verify that all missing values have been filled
print("Missing values after KNN imputation:")
print(train_quantitative['MRI_Track_Age_at_Scan'].isna().sum())

# Plot the new distribution
sns.histplot(train_quantitative['MRI_Track_Age_at_Scan'], kde=True)
plt.title("Distribution of MRI_Track_Age_at_Scan (After KNN Imputation)")
plt.xlabel("Age at Scan")
plt.show()

# test quantitative explore missing data
test_quantitative_missing_val = test_quantitative.isna().sum()
print({"Missing values for test quantitative is \n": test_quantitative_missing_val})

num_columns = test_quantitative.select_dtypes(include=['float64', 'int64'])

# checking the distribution of each missing values columns before applying KNN
# imputation
for col in num_columns:
  plt.figure(figsize=(8, 6))
  sns.histplot(test_quantitative[col], kde=True)
  plt.title(f"Distribution of {col}")
  plt.xlabel(col)
  plt.show()

imputer = KNNImputer(n_neighbors=5)

# Apply KNN imputation (excluding participant_id)
test_quantitative.iloc[:, 1:] = imputer.fit_transform(test_quantitative.iloc[:, 1:])

# check the distribution again after imputation
for col in num_columns:
  plt.figure(figsize=(8, 6))
  sns.histplot(test_quantitative[col], kde=True)
  plt.title(f"Distribution of {col}")
  plt.xlabel(col)
  plt.show()

test_quantitative_missing_val = test_quantitative.isna().sum()
print({"Missing values for test quantitative is \n": test_quantitative_missing_val})

print("Checking train quantitative")
print(train_quantitative.isnull().sum())
print(train_quantitative.describe())
train_quantitative.describe()

test_quantitative.describe()

"""### 2. train_categorical and test_categorical dataset

to clean: train_categorical.
The TRAIN_CATEGORICAL_METADATA dataset contains categorical metadata for training participants. This dataset helps the model learn patterns associated with ADHD diagnosis and sex differences based on demographic and socio-environmental factors.

#### Train_Categorical
"""

train_categorical.head()

train_categorical.columns

train_categorical['PreInt_Demos_Fam_Child_Ethnicity'].dtype

print("Missing values before imputation:", train_categorical.isnull().sum())

# Check if any missing values were present before filling
print("Missing values before imputation:", train_categorical.isnull().sum())

nan_count_train = np.sum(train_categorical.isnull(), axis = 0)
nan_count_train

# nan_detected tells us that a column has null values, not how many
nan_detected_train = nan_count_train!=0
nan_detected_train

is_int_or_float_train = (train_categorical.dtypes == 'int64') | (train_categorical.dtypes == 'float64')
is_int_or_float_train

# to_impute will tell us if a given feature is both of numeric data AND has missing values
to_impute_train = nan_detected_train & is_int_or_float_train
to_impute_train

to_impute_selected_train = train_categorical.columns[to_impute_train].tolist()
to_impute_selected_train

for colname_train in to_impute_selected_train:
    train_categorical[colname_train].fillna(np.mean(train_categorical[colname_train]), inplace=True)

for colname_train in to_impute_selected_train:
    print("{} missing values count :{}".format(colname_train, np.sum(train_categorical[colname_train].isnull(), axis = 0)))

nan_count_train = np.sum(test_categorical.isnull(), axis = 0)
nan_count_train

# Plot the new distribution
sns.histplot(test_categorical['PreInt_Demos_Fam_Child_Ethnicity'], kde=True)
plt.title("Distribution of PreInt_Demos_Fam_Child_Ethnicity in Train Dataset")
plt.xlabel("Age at Scan")
plt.show()

"""#### Test_Categorial

only participant ID is an object, and that's arbitrary, so we **don't** need to implement one-hot encoding on it.


next step: finding numerical outliers, and scaling / normalizing data!
"""

test_categorical.dtypes

df = test_categorical # copy, also short
import seaborn as sns
fig = sns.histplot(df, x='Basic_Demos_Enroll_Year')

test_categorical.head()

test_categorical.nunique()

nan_count_test = np.sum(test_categorical.isnull(), axis = 0)
nan_count_test

# same process as before;
# Plot the new distribution
sns.histplot(test_categorical['PreInt_Demos_Fam_Child_Ethnicity'], kde=True)
plt.title("Distribution of PreInt_Demos_Fam_Child_Ethnicity in Test Dataset (Before)")
plt.xlabel("Age at Scan")
plt.show()

# nan_detected tells us that a column has null values, not how many
nan_detected_test = nan_count_test!=0
nan_detected_test

is_int_or_float_test = (test_categorical.dtypes == 'int64') | (test_categorical.dtypes == 'float64')
is_int_or_float_test

# to_impute will tell us if a given feature is both of numeric data AND has missing values
to_impute_test = nan_detected_test & is_int_or_float_test
to_impute_test


to_impute_selected_test = test_categorical.columns[to_impute_test].tolist()
to_impute_selected_test

# Plot the new distribution
sns.histplot(test_categorical['PreInt_Demos_Fam_Child_Ethnicity'], kde=True)
plt.title("Distribution of PreInt_Demos_Fam_Child_Ethnicity Test Dataset (After)")
plt.xlabel("Age at Scan")
plt.show()

for colname_test in to_impute_selected_test:
    test_categorical[colname_test +'_na'] = test_categorical[colname_test].isnull()
test_categorical.head()

for colname_test in to_impute_selected_test:
    test_categorical[colname_test].fillna(3, inplace=True)

for colname_test in to_impute_selected_test:
    print("{} missing values count :{}".format(colname_test, np.sum(test_categorical[colname_test].isnull(), axis = 0)))

"""### 3. train_connectome and test_connectome dataset

#### Train_Connectome
"""

print(f"Current rows in train_connectome: {len(train_connectome)}")

train_connectome.head()

# check for missing values
print(train_connectome.isnull().sum())
train_connectome.dropna(inplace=True)

# check data types
train_connectome.dtypes

train_connectome.duplicated().sum()
print(train_connectome.shape)
print("Check")
print(train_connectome.isnull().sum())

"""#### Test_Connectome"""

# Display basic info
test_connectome.head()

# calculate number of missing values in each column
# show columns with these values
null_vals = test_connectome.isnull().sum();
print(null_vals[null_vals > 0])

test_connectome.dropna(inplace = True)

"""###3. train_solution dataset

checking if there is any invalid data
"""

# train_solutions.isnull().sum()
train_solutions.isna().sum()

train_solutions.describe()

train_solutions.head()

train_solutions.dtypes

train_categorical.dtypes

train_connectome.dtypes

train_quantitative.dtypes

"""###MERGE TRAINING DATASETS"""

print(train_connectome.head)
print(train_categorical.head)
print(train_quantitative.head)
print(train_solutions.head)

print("Check categorical")
print(train_categorical.shape)

print(train_solutions.shape)

print(train_connectome.shape)

print("Check quantitative")
print(train_quantitative.shape)

# train_pca_connectome =  pd.read_csv('/content/drive/MyDrive/Spring AI Studio Team 24 WiDS/TRAIN/train_pca.csv')
train_merged = (
    train_connectome
    .merge(train_categorical, on='participant_id', how='left')
    .merge(train_quantitative, on='participant_id', how='left')
    .merge(train_solutions, on='participant_id', how='left')
)

# # Save the merged dataset
# train_merged.to_csv('/content/drive/MyDrive/Spring AI Studio Team 24 WiDS/TRAIN/TRAIN_MERGED.csv', index=False)

# Display first rows of data
print(train_merged.head())

"""###MERGE TEST DATASETS"""

test_merged = (
    test_categorical
    .merge(test_quantitative, on='participant_id', how='left')
    .merge(test_connectome, on='participant_id', how='left')
)

# Save the merged dataset
# test_merged.to_csv('/content/drive/MyDrive/Spring AI Studio Team 24 WiDS/TEST/TEST_MERGED.csv', index=False)

# Display first rows of data
print(test_merged.head())
# Check if there are missing values
print(" Missing values in test_merged:")
print(test_merged.isna().sum()[test_merged.isna().sum() > 0])

"""##**3. Model Training**

"""

# Ensure Consistent Features Between Train & Test**
train_features = set(train_merged.columns) - {'ADHD_Outcome', 'Sex_F'}
test_features = set(test_merged.columns)

missing_in_test = train_features - test_features
extra_in_test = test_features - train_features

print("Features in train but missing in test:", missing_in_test)
print("Features in test but missing in train:", extra_in_test)

# Ensure `test_merged` has the same features as `train_merged` (excluding targets)
test_merged = test_merged[list(train_features)]

# Define Features & Targets
X = train_merged.drop(columns=['participant_id', 'ADHD_Outcome', 'Sex_F'])
y_adhd = train_merged['ADHD_Outcome']
y_sex = train_merged['Sex_F']

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import VarianceThreshold, mutual_info_classif
from sklearn.metrics import f1_score
from sklearn.ensemble import RandomForestClassifier, StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier
import lightgbm as lgb
from imblearn.over_sampling import SMOTE
import optuna
# from catboost import CatBoostClassifier  # Optional

# -------------------------------
# Enhanced Preprocessing Function
# -------------------------------
def preprocess_for_sex_enhanced(df):
    df = df.copy()

    # Add domain-informed features
    df['SDQ_Emo_Score'] = df['SDQ_SDQ_Emotional_Problems'] + df['SDQ_SDQ_Internalizing'] + df['SDQ_SDQ_Peer_Problems']
    df['APQ_Parenting_Balance'] = df['APQ_P_APQ_P_PP'] - df['APQ_P_APQ_P_CP'] - df['APQ_P_APQ_P_PM']
    df['Emo_Hyper_Diff'] = df['SDQ_SDQ_Hyperactivity'] - df['SDQ_SDQ_Emotional_Problems']
    df['Age_Squared'] = df['MRI_Track_Age_at_Scan'] ** 2

    df['MRI_Age_Bucket'] = pd.cut(df['MRI_Track_Age_at_Scan'], bins=[3, 7, 12, 17, 20],
                                  labels=['young', 'child', 'teen', 'youngadult'])

    categorical_cols = ['PreInt_Demos_Fam_Child_Ethnicity', 'PreInt_Demos_Fam_Child_Race', 'MRI_Age_Bucket']
    df = pd.get_dummies(df, columns=categorical_cols, drop_first=False)

    return df

# -------------------------------
# Optuna Objective Function
# -------------------------------
def optuna_objective(trial, X, y):
    params = {
        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.2, log=True),
        'max_depth': trial.suggest_int('max_depth', 3, 15),
        'n_estimators': trial.suggest_int('n_estimators', 100, 1000, step=100),
        'subsample': trial.suggest_float('subsample', 0.6, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
        'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 2.0),
        'reg_alpha': trial.suggest_float('reg_alpha', 0.1, 2.0),
        'num_leaves': trial.suggest_int('num_leaves', 20, 300, step=10),
        'min_child_samples': trial.suggest_int('min_child_samples', 15, 50),
        'scale_pos_weight': trial.suggest_float('scale_pos_weight', 0.5, 5.0)
    }

    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    f1_scores = []
    for train_idx, val_idx in skf.split(X, y):
        X_train, X_val = X[train_idx], X[val_idx]
        y_train, y_val = y[train_idx], y[val_idx]
        model = lgb.LGBMClassifier(objective='binary', **params)
        model.fit(X_train, y_train)
        preds = model.predict(X_val)
        f1_scores.append(f1_score(y_val, preds))
    return np.mean(f1_scores)

# -------------------------------
# Threshold Tuning
# -------------------------------
def tune_threshold(y_true, y_probs, pos_label=1):
    best_thresh, best_f1 = 0.5, 0
    for t in np.arange(0.2, 0.8, 0.01):
        preds = (y_probs > t).astype(int)
        f1 = f1_score(y_true, preds, pos_label=pos_label)
        if f1 > best_f1:
            best_f1 = f1
            best_thresh = t
    return best_thresh, best_f1

# -------------------------------
# Preprocess and Split Data
# -------------------------------
sex_df = preprocess_for_sex_enhanced(train_merged)
sex_features = [col for col in sex_df.columns if col not in ['participant_id', 'ADHD_Outcome', 'Sex_F']]
X_sex = sex_df[sex_features]
y_sex = sex_df['Sex_F']

# Optional: Check feature importance
mi = mutual_info_classif(X_sex, y_sex)
mi_series = pd.Series(mi, index=X_sex.columns).sort_values(ascending=False)
print("üîç Top Mutual Info Features:\n", mi_series.head(10))

# Feature reduction and scaling
var_sex = VarianceThreshold(0.02)
X_sex_red = var_sex.fit_transform(X_sex)
scaler_sex = StandardScaler()
X_sex_scaled = scaler_sex.fit_transform(X_sex_red)

# Split and balance
X_train_sex, X_val_sex, y_train_sex, y_val_sex = train_test_split(
    X_sex_scaled, y_sex, test_size=0.2, stratify=y_sex, random_state=42)

X_train_sex_sm, y_train_sex_sm = SMOTE(random_state=42).fit_resample(X_train_sex, y_train_sex)

# -------------------------------
# Optuna Hyperparameter Tuning
# -------------------------------
study_sex = optuna.create_study(direction='maximize')
study_sex.optimize(lambda trial: optuna_objective(trial, X_sex_scaled, y_sex.values), n_trials=100)
best_sex_params = study_sex.best_params

# -------------------------------
# Build Final Stacking Model
# -------------------------------
stack_sex = StackingClassifier(
    estimators=[
        ('lgbm', lgb.LGBMClassifier(objective='binary', **best_sex_params)),
        ('rf', RandomForestClassifier(n_estimators=500, class_weight='balanced', random_state=42)),
        ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42))
        # ('catboost', CatBoostClassifier(verbose=0))  # Uncomment if you want to try CatBoost
    ],
    final_estimator=lgb.LGBMClassifier(class_weight='balanced', random_state=42),
    cv=5
)

stack_sex.fit(X_train_sex_sm, y_train_sex_sm)

# -------------------------------
# Evaluate on Validation Set
# -------------------------------
y_probs_sex = stack_sex.predict_proba(X_val_sex)[:, 1]
best_thresh_sex, best_f1_sex = tune_threshold(y_val_sex, y_probs_sex)
print(f"üë© Improved Sex F1 Score: {best_f1_sex:.4f} @ threshold={best_thresh_sex:.2f}")
